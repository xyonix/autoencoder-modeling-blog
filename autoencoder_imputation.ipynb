{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e334f3-e674-4c3d-9f79-bf94d29b79bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>:root { --jp-notebook-max-width: 100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fad5c5-e2cd-4f69-bbfd-fd0856299b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from types import SimpleNamespace\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, Concatenate, ReLU\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers.legacy import Adam  # Use legacy optimizer for M1/M2 Macs\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.layers import Masking\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.saving import register_keras_serializable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5df8b4-5160-4f4a-a853-dc1bf9ce8d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR=os.path.abspath(os.path.expanduser(\"models\"))\n",
    "\n",
    "cfg = SimpleNamespace(\n",
    "    NA_FILL=-1,\n",
    "    AUTO_ENCODER_EPOCHS=100,\n",
    "    AUTO_ENCODER_BATCH_SIZE=32,\n",
    "    AUTO_ENCODER_LEARNING_RATE=0.001,\n",
    "    MODEL_DIR=MODEL_DIR,\n",
    "    AUTOENCODER_MODEL_PATH=os.path.join(MODEL_DIR, \"autoencoder_model.keras\"),\n",
    "    RF_MODEL_PATH=os.path.join(MODEL_DIR, \"rf_imputer.joblib\"),\n",
    "    OVERWRITE_AUTOENCODER=False,\n",
    "    OVERWRITE_RF=False,\n",
    "    VERBOSE=1,\n",
    "    RANDOM_STATE=100,\n",
    "    RANDOM_SAMPLE_FRACTION=0.2,\n",
    "    TEST_SIZE=0.2,\n",
    "    HISTORY_FILE_PATH=os.path.join(MODEL_DIR, \"autoencoder_training_history.json\")\n",
    ")\n",
    "\n",
    "os.makedirs(cfg.MODEL_DIR, exist_ok=True)\n",
    "\n",
    "cfg.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427e0a14-0d3e-4ab8-8b9a-b9f0df131603",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable()  # Register the custom layer\n",
    "class NonNegativeOutputLayer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        fixed_outputs = tf.nn.relu(inputs[:, :num_fixed_features])  # Enforce non-negative values for fixed features\n",
    "        unrestricted_outputs = inputs[:, num_fixed_features:]  # Allow unrestricted values for lat/lon\n",
    "        return tf.concat([fixed_outputs, unrestricted_outputs], axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()  # Get base config if any\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5dbf5c-9944-46ce-af3e-09c28756997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and simulate missing values\n",
    "data = fetch_california_housing()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# Save original values for comparison\n",
    "original_df = df.copy()\n",
    "\n",
    "# Introduce NaNs randomly, excluding Longitude and Latitude\n",
    "np.random.seed(cfg.RANDOM_STATE)\n",
    "nan_mask = np.random.rand(*df.shape) < cfg.RANDOM_SAMPLE_FRACTION\n",
    "nan_mask[:, df.columns.get_loc('Longitude')] = False  # Exclude Longitude\n",
    "nan_mask[:, df.columns.get_loc('Latitude')] = False   # Exclude Latitude\n",
    "df[nan_mask] = np.nan\n",
    "\n",
    "# Separate rows with and without missing values\n",
    "train_df = df.dropna()\n",
    "test_df = df[df.isna().any(axis=1)]\n",
    "\n",
    "# Normalize and handle missing values\n",
    "df_filled = df.fillna(cfg.NA_FILL)\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_filled), columns=df.columns)\n",
    "\n",
    "# Scale train and test sets individually for model input\n",
    "X_train = scaler.transform(train_df.fillna(cfg.NA_FILL))\n",
    "X_test = scaler.transform(test_df.fillna(cfg.NA_FILL))\n",
    "\n",
    "# Define fixed and unrestricted features\n",
    "fixed_features = [col for col in df.columns if col not in [\"Longitude\", \"Latitude\"]]\n",
    "num_fixed_features = len(fixed_features)\n",
    "\n",
    "# Learning rate warm-up function\n",
    "initial_lr = 1e-5\n",
    "def warmup(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr + (initial_lr * 0.2)\n",
    "    return lr\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train.shape[1]\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "# Encoder\n",
    "encoded = Dense(256, activation=\"relu\")(input_layer)\n",
    "encoded = LayerNormalization()(encoded)\n",
    "encoded = Dropout(0.2)(encoded)\n",
    "encoded = Dense(128, activation=\"relu\")(encoded)\n",
    "encoded = LayerNormalization()(encoded)\n",
    "encoded = Dropout(0.2)(encoded)\n",
    "encoded = Dense(64, activation=\"relu\")(encoded)\n",
    "encoded = LayerNormalization()(encoded)\n",
    "encoded = Dropout(0.2)(encoded)\n",
    "encoded = Dense(32, activation=\"relu\")(encoded)\n",
    "\n",
    "# Decoder\n",
    "decoded = Dense(64, activation=\"relu\")(encoded)\n",
    "decoded = LayerNormalization()(decoded)\n",
    "decoded = Dropout(0.2)(decoded)\n",
    "decoded = Dense(128, activation=\"relu\")(decoded)\n",
    "decoded = LayerNormalization()(decoded)\n",
    "decoded = Dropout(0.2)(decoded)\n",
    "decoded = Dense(256, activation=\"relu\")(decoded)\n",
    "decoded = LayerNormalization()(decoded)\n",
    "decoded = Dropout(0.2)(decoded)\n",
    "\n",
    "# Final output with custom non-negative layer\n",
    "output_layer = Dense(input_dim)(decoded)\n",
    "output_layer = NonNegativeOutputLayer()(output_layer)  # Apply custom layer to enforce constraints\n",
    "\n",
    "# Model definition\n",
    "autoencoder = Model(input_layer, output_layer)\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=initial_lr), loss=\"mse\")\n",
    "\n",
    "# Callbacks\n",
    "history_file_path = cfg.HISTORY_FILE_PATH\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=1e-6)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_warmup = LearningRateScheduler(warmup, verbose=1)\n",
    "\n",
    "# Training with saving and loading history\n",
    "if cfg.OVERWRITE_AUTOENCODER or not os.path.exists(cfg.AUTOENCODER_MODEL_PATH):\n",
    "    print(\"Training autoencoder with enhanced configuration...\")\n",
    "    history = autoencoder.fit(\n",
    "        X_train, X_train,\n",
    "        epochs=cfg.AUTO_ENCODER_EPOCHS,\n",
    "        batch_size=cfg.AUTO_ENCODER_BATCH_SIZE,\n",
    "        validation_data=(X_test, X_test),\n",
    "        verbose=cfg.VERBOSE,\n",
    "        callbacks=[lr_warmup, lr_reduction, early_stopping]\n",
    "    )\n",
    "    autoencoder.save(cfg.AUTOENCODER_MODEL_PATH)\n",
    "\n",
    "    # Save training history to disk\n",
    "    with open(history_file_path, 'w') as f:\n",
    "        json.dump({k: [float(v) for v in values] for k, values in history.history.items()}, f)\n",
    "    history_data = history.history  # Save for plotting\n",
    "else:\n",
    "    print(\"Loading pre-trained autoencoder...\")\n",
    "    # autoencoder = load_model(cfg.AUTOENCODER_MODEL_PATH)\n",
    "    autoencoder = load_model(\n",
    "        cfg.AUTOENCODER_MODEL_PATH,\n",
    "        custom_objects={\"NonNegativeOutputLayer\": NonNegativeOutputLayer},\n",
    "        compile=False\n",
    "    )\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=initial_lr), loss=\"mse\")\n",
    "    with open(history_file_path, 'r') as f:\n",
    "        history_data = json.load(f)\n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history_data['loss'], label='Training Loss')\n",
    "plt.plot(history_data['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Autoencoder Training and Validation Loss Over Epochs')\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Impute missing values using the autoencoder\n",
    "print(\"Imputing missing values using the autoencoder...\")\n",
    "X_test_pred = autoencoder.predict(X_test)\n",
    "pred_df = pd.DataFrame(scaler.inverse_transform(X_test_pred), columns=df.columns, index=test_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f76f270-1c56-449d-9a5a-25cfba41427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee98e50-8cd3-4882-919f-0a5c82200e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_embedding_layer(model):\n",
    "    previous_neurons = float('inf')\n",
    "    embedding_layer_index = None\n",
    "    \n",
    "    # Start by skipping the input layer\n",
    "    for i, layer in enumerate(model.layers[1:], start=1):  # Skip index 0\n",
    "        output_shape = layer.output_shape[0] if isinstance(layer.output_shape, list) else layer.output_shape\n",
    "        if len(output_shape) > 1:  # Make sure it's a valid tensor shape\n",
    "            num_neurons = output_shape[1]\n",
    "            \n",
    "            # Check if we've hit the point of increase, marking the end of encoding\n",
    "            if num_neurons > previous_neurons:\n",
    "                embedding_layer_index = i - 1  # The last smallest layer is the embedding layer\n",
    "                break\n",
    "            \n",
    "            previous_neurons = num_neurons\n",
    "\n",
    "    return model.layers[embedding_layer_index]\n",
    "\n",
    "\n",
    "def get_embeddings(autoencoder, input_df, scaler):\n",
    "    # Normalize the entire input DataFrame\n",
    "    normalized_input = scaler.transform(input_df)\n",
    "\n",
    "    # Find the embedding layer\n",
    "    embedding_layer = find_embedding_layer(autoencoder)\n",
    "    \n",
    "    # Create a model up to the embedding layer\n",
    "    encoder_model = Model(inputs=autoencoder.input, outputs=embedding_layer.output)\n",
    "    \n",
    "    # Predict the embeddings for all rows without verbose output\n",
    "    embeddings = encoder_model.predict(normalized_input, verbose=0)\n",
    "    \n",
    "    return pd.DataFrame(embeddings, index=input_df.index)\n",
    "\n",
    "\n",
    "data = original_df.sample(3)\n",
    "embedding_vectors_df = get_embeddings(autoencoder, data, scaler)\n",
    "\n",
    "display(data)\n",
    "display(embedding_vectors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1340ad14-0549-4ac7-9a37-8707cef86116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "\n",
    "if cfg.OVERWRITE_RF or not os.path.exists(cfg.RF_MODEL_PATH):\n",
    "\n",
    "    print(\"Imputing missing values using IterativeImputer with Random Forest...\")\n",
    "    \n",
    "    # Fit the imputer on X_train (complete cases only)\n",
    "    rf_imputer = IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=cfg.RANDOM_STATE)\n",
    "    rf_imputer.fit(X_train)  # Train only on non-missing data\n",
    "\n",
    "    # Save the fitted IterativeImputer with RandomForest\n",
    "    joblib.dump(rf_imputer, cfg.RF_MODEL_PATH)\n",
    "    print(\"Imputer model saved to disk.\")\n",
    "\n",
    "else:\n",
    "    print('Loading RF imputation model from disk')\n",
    "    rf_imputer = joblib.load(cfg.RF_MODEL_PATH)\n",
    "    \n",
    "# Use the trained imputer to transform X_test, applying the learned imputation patterns\n",
    "rf_imputed_scaled = rf_imputer.transform(X_test)\n",
    "rf_imputed = pd.DataFrame(scaler.inverse_transform(rf_imputed_scaled), columns=df.columns, index=test_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df40c1d-fe0c-477c-8bdd-3ac29db9ff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 7: Calculate absolute differences between imputed values and original values for the test set\n",
    "abs_diff_records = []\n",
    "\n",
    "for col in test_df.columns:\n",
    "    # Get indices in the test set where original data had NaNs\n",
    "    missing_indices = test_df.loc[test_df[col].isna(), col].index\n",
    "\n",
    "    # Extract the relevant values from original, autoencoder, and random forest imputed DataFrames\n",
    "    original_values = original_df.loc[missing_indices, col]\n",
    "    ae_imputed_values = pred_df.loc[missing_indices, col]\n",
    "    rf_imputed_values = rf_imputed.loc[missing_indices, col]\n",
    "    \n",
    "    # Calculate absolute differences for each model, avoiding potential NaN comparisons\n",
    "    ae_abs_diff = (ae_imputed_values - original_values).abs()\n",
    "    rf_abs_diff = (rf_imputed_values - original_values).abs()\n",
    "    \n",
    "    # Append results for plotting\n",
    "    abs_diff_records.extend([\n",
    "        {'Feature': col, 'Model': 'Autoencoder', 'Abs Difference': diff}\n",
    "        for diff in ae_abs_diff.dropna()\n",
    "    ])\n",
    "    abs_diff_records.extend([\n",
    "        {'Feature': col, 'Model': 'Random Forest', 'Abs Difference': diff}\n",
    "        for diff in rf_abs_diff.dropna()\n",
    "    ])\n",
    "\n",
    "# Convert to DataFrame for analysis and plotting\n",
    "abs_diff_df = pd.DataFrame(abs_diff_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a01d0e-6849-410f-93c1-8340b3d16c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a dark theme and define a color palette\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.set_context(\"talk\", font_scale=1.1)\n",
    "plt.rcParams['font.family'] = 'Avenir'\n",
    "custom_palette = {\"Autoencoder\": \"cornflowerblue\", \"Random Forest\": \"#ff7f0e\"}  # Vibrant colors for models\n",
    "\n",
    "# Create the box plot with enhanced styling\n",
    "golden_ratio = 1.6180339887\n",
    "plt.figure(figsize=(14, 14/golden_ratio))\n",
    "sns.boxplot(\n",
    "    data=abs_diff_df, \n",
    "    x='Feature', \n",
    "    y='Abs Difference', \n",
    "    hue='Model', \n",
    "    palette=custom_palette,\n",
    "    width=0.5,  # Narrower boxes\n",
    "    linewidth=1.5,  # Thicker lines for contrast\n",
    "    fliersize=3  # Smaller outliers\n",
    ")\n",
    "\n",
    "# Log scale for the y-axis and adjusted tick parameters\n",
    "fontsize=18\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(rotation=45, fontsize=fontsize)\n",
    "plt.yticks(color=\"black\")\n",
    "\n",
    "# Titles and labels with color adjustments\n",
    "plt.title(\"Absolute Differences Between Imputed and Original Values by Feature and Model\", fontsize=fontsize*1.2, fontweight='bold', \n",
    "          loc='left')\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"absolute difference (log scale)\", color=\"black\", fontsize=fontsize)\n",
    "plt.legend(title=\"\", fontsize=14, title_fontsize=12, facecolor='white', framealpha=0.8)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64250b00-6355-4a07-8e39-9fc890aa3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_abs_diff_df = abs_diff_df.groupby(['Feature', 'Model']).mean().reset_index()\n",
    "\n",
    "# Pivot the DataFrame for easier comparison\n",
    "pivot_df = mean_abs_diff_df.pivot(index='Feature', columns='Model', values='Abs Difference')\n",
    "\n",
    "# Calculate the ratio of the Abs Difference for Random Forest to Autoencoder\n",
    "pivot_df['RF/AE'] = pivot_df['Random Forest'] / pivot_df['Autoencoder']\n",
    "\n",
    "# Display the result with only the 'Ratio' column\n",
    "ratio_df = pivot_df[['RF/AE']].reset_index()\n",
    "ratio_df.columns.name = None\n",
    "\n",
    "ratio_df.sort_values('RF/AE', ascending=False, inplace=True)\n",
    "\n",
    "display(ratio_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec230f-2371-4ecf-bce7-b15210f645d3",
   "metadata": {},
   "source": [
    "# supervised learning\n",
    "\n",
    "Use the encoder to get the embedding vectors related to the training data. \n",
    "Train two models: one with the orignal data and one with the emebdding vectors: is there a difference in efficacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf534b2f-daca-41b7-a0a1-e76708591384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_and_evaluate_rf(df, target, test_size=0.2, random_state=42, n_estimators=100):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Random Forest regressor on the given data.\n",
    "\n",
    "    @param df: DataFrame containing feature data.\n",
    "    @param target: Series or array-like object containing the target variable.\n",
    "    @param test_size: float, proportion of data to be used for testing.\n",
    "    @param random_state: int, seed for reproducibility.\n",
    "    @param n_estimators: int, number of trees in the forest.\n",
    "\n",
    "    @return: tuple (mse, r2, rf_regressor) containing Mean Squared Error, R-squared, and trained model.\n",
    "    \"\"\"\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Create a Random Forest regressor\n",
    "    rf_regressor = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state)\n",
    "    \n",
    "    # Train the model\n",
    "    rf_regressor.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = rf_regressor.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    return mse, r2, rf_regressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c334883e-fb2b-473f-9eee-1e1a0a23f69e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb37f50-f2c7-4419-86ae-75aeee5ee248",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators=100\n",
    "\n",
    "# Load the California housing data\n",
    "data = fetch_california_housing()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "target = data.target\n",
    "\n",
    "# regression on regular data\n",
    "mse, r2, rf_regressor = train_and_evaluate_rf(df, target, test_size=cfg.TEST_SIZE, random_state=cfg.RANDOM_STATE, n_estimators=n_estimators)\n",
    "\n",
    "print(\"REGULAR DATA\")\n",
    "print(f\"Mean Squared Error (MSE): {round(mse, 2)}\")\n",
    "print(f\"R-squared (R2): {round(r2, 2)}\")\n",
    "\n",
    "# regression on transformed data into embeddings via the autoencoder\n",
    "ae_embed_df = get_embeddings(autoencoder, df, scaler)\n",
    "mse_ae, r2_ae, rf_regressor_ae = train_and_evaluate_rf(ae_embed_df, target, test_size=cfg.TEST_SIZE, random_state=cfg.RANDOM_STATE, n_estimators=n_estimators)\n",
    "\n",
    "print(\"\\nAUTOENCODED EMBEDDINGS\")\n",
    "print(f\"Mean Squared Error (MSE): {round(mse_ae, 2)}\")\n",
    "print(f\"R-squared (R2): {round(r2_ae, 2)}\")\n",
    "\n",
    "# regression on regular data augmented with AE embeddings\n",
    "aug_ae_df = pd.concat([df, ae_embed_df], axis=1)\n",
    "aug_ae_df.columns = [str(s) for s in aug_ae_df.columns]\n",
    "mse_aug_ae, r2_aug_ae, rf_aug_regressor_ae = train_and_evaluate_rf(aug_ae_df, target, test_size=cfg.TEST_SIZE, random_state=cfg.RANDOM_STATE, n_estimators=n_estimators)\n",
    "print(\"\\nREGULAR DATA AUGMENTED WITH AUTOENCODED EMBEDDINGS\")\n",
    "print(f\"Mean Squared Error (MSE): {round(mse_aug_ae, 2)}\")\n",
    "print(f\"R-squared (R2): {round(r2_aug_ae, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c0dfc6-37bb-43e2-bbaf-cf2a463482f0",
   "metadata": {},
   "source": [
    "The results indicate that the Random Forest regression model trained on the original data performs significantly better than the model trained on the autoencoder embeddings. Here’s what we can infer:\n",
    "\n",
    "\t1.\tPerformance Metrics:\n",
    "\t•\tThe original data model has a lower Mean Squared Error (MSE) of 0.255, indicating that its predictions are closer to the actual target values compared to the model using the autoencoder embeddings, which has a higher MSE of 0.454.\n",
    "\t•\tThe R-squared (R2) score for the original data model is 0.805, showing that it explains about 80.5% of the variance in the target variable. The AE model has a lower R-squared score of 0.654, explaining only 65.4% of the variance.\n",
    "\t2.\tInterpretation of Autoencoder Embeddings:\n",
    "\t•\tThe embeddings generated by the autoencoder capture compressed representations of the original data. While these embeddings may retain important structural information, they are inherently a reduced form of the input features. This reduction can lead to a loss of fine-grained details that are critical for a regression task, causing the model to underperform compared to using the original data.\n",
    "\t•\tThe lower R-squared score of the AE model suggests that the embeddings might not capture all relevant information necessary for the Random Forest model to make precise predictions.\n",
    "\t3.\tImplications:\n",
    "\t•\tThe original data is better suited for training the Random Forest model in this case, as it retains all feature details needed for accurate prediction.\n",
    "\t•\tThe autoencoder’s embeddings might still be useful for dimensionality reduction, noise reduction, or other tasks, but for this regression problem, they seem to compromise model performance.\n",
    "\t4.\tPotential Improvements:\n",
    "\t•\tFine-tuning the architecture of the autoencoder, adjusting its latent space size, or training it to retain more critical information could improve the model’s efficacy.\n",
    "\t•\tAlternatively, using different embedding techniques or combining original data features with embeddings may yield better results.\n",
    "\n",
    "In summary, while the autoencoder-based model captures some structural information, the reduced dimensionality results in a loss of detail, leading to reduced performance compared to the model trained on the full original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfc1964-3fef-40da-b51c-80b59c76601e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
