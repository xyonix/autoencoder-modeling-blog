{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cd7259",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".container {\n",
    "    width: 100% !important;\n",
    "    max-width: 100% !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fad5c5-e2cd-4f69-bbfd-fd0856299b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, Concatenate, ReLU\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "# from tensorflow.keras.optimizers.legacy import Adam  # Use legacy optimizer for M1/M2 Macs\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.layers import Masking\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.saving import register_keras_serializable\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205d6e53-650e-4c7d-9a37-78f9e04c190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local functions for display\n",
    "\n",
    "def header(text, level=2, color='cornflowerblue'):\n",
    "    display(HTML(f'<h{level} style=\"color:{color};\">{text}</h{level}>'))\n",
    "\n",
    "def full_display(data: pd.DataFrame, index: bool = False):\n",
    "    display(HTML(data.to_html(index=index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5df8b4-5160-4f4a-a853-dc1bf9ce8d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define configuration\n",
    "\n",
    "MODEL_DIR=os.path.abspath(os.path.expanduser(\"models\"))\n",
    "\n",
    "cfg = SimpleNamespace(\n",
    "    NA_FILL=-1,\n",
    "    AUTO_ENCODER_EPOCHS=100,\n",
    "    AUTO_ENCODER_BATCH_SIZE=32,\n",
    "    AUTO_ENCODER_LEARNING_RATE=0.001,\n",
    "    MODEL_DIR=MODEL_DIR,\n",
    "    AUTOENCODER_MODEL_PATH=os.path.join(MODEL_DIR, \"autoencoder_model.keras\"),\n",
    "    RF_MODEL_PATH=os.path.join(MODEL_DIR, \"rf_imputer.joblib\"),\n",
    "    OVERWRITE_AUTOENCODER=True,\n",
    "    OVERWRITE_RF=False,\n",
    "    VERBOSE=1,\n",
    "    RANDOM_STATE=100,\n",
    "    RANDOM_SAMPLE_FRACTION=0.2,\n",
    "    TEST_SIZE=0.2,\n",
    "    HISTORY_FILE_PATH=os.path.join(MODEL_DIR, \"autoencoder_training_history.json\")\n",
    ")\n",
    "\n",
    "os.makedirs(cfg.MODEL_DIR, exist_ok=True)\n",
    "\n",
    "cfg.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427e0a14-0d3e-4ab8-8b9a-b9f0df131603",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable()  # Register the custom layer\n",
    "class NonNegativeOutputLayer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        fixed_outputs = tf.nn.relu(inputs[:, :num_fixed_features])  # Enforce non-negative values for fixed features\n",
    "        unrestricted_outputs = inputs[:, num_fixed_features:]  # Allow unrestricted values for lat/lon\n",
    "        return tf.concat([fixed_outputs, unrestricted_outputs], axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()  # Get base config if any\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914b4e92-26ba-4a6f-852e-312acd4efec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_california_housing()\n",
    "data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5dbf5c-9944-46ce-af3e-09c28756997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and simulate missing values\n",
    "data = fetch_california_housing()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "target = data.target\n",
    "\n",
    "# Save original values for comparison\n",
    "original_df = df.copy()\n",
    "\n",
    "# Introduce NaNs randomly, excluding Longitude and Latitude\n",
    "np.random.seed(cfg.RANDOM_STATE)\n",
    "nan_mask = np.random.rand(*df.shape) < cfg.RANDOM_SAMPLE_FRACTION\n",
    "nan_mask[:, df.columns.get_loc('Longitude')] = False  # Exclude Longitude\n",
    "nan_mask[:, df.columns.get_loc('Latitude')] = False   # Exclude Latitude\n",
    "df[nan_mask] = np.nan\n",
    "\n",
    "# Separate rows with and without missing values\n",
    "train_df = df.dropna()\n",
    "test_df = df[df.isna().any(axis=1)]\n",
    "\n",
    "df_filled = df.fillna(cfg.NA_FILL)\n",
    "\n",
    "# Normalize and handle missing values\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data\n",
    "scaler.fit(df_filled)\n",
    "\n",
    "# Transform the data\n",
    "df_scaled = pd.DataFrame(scaler.transform(df_filled), columns=df_filled.columns)\n",
    "\n",
    "# Scale train and test sets individually for model input\n",
    "X_train = scaler.transform(train_df.fillna(cfg.NA_FILL))\n",
    "X_test = scaler.transform(test_df.fillna(cfg.NA_FILL))\n",
    "\n",
    "# Define fixed and unrestricted features\n",
    "fixed_features = [col for col in df.columns if col not in [\"Longitude\", \"Latitude\"]]\n",
    "num_fixed_features = len(fixed_features)\n",
    "\n",
    "# Learning rate warm-up function\n",
    "initial_lr = 1e-5\n",
    "def warmup(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr + (initial_lr * 0.2)\n",
    "    return lr\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train.shape[1]\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "# Encoder\n",
    "encoded = Dense(256, activation=\"relu\")(input_layer)\n",
    "encoded = LayerNormalization()(encoded)\n",
    "encoded = Dropout(0.2)(encoded)\n",
    "encoded = Dense(128, activation=\"relu\")(encoded)\n",
    "encoded = LayerNormalization()(encoded)\n",
    "encoded = Dropout(0.2)(encoded)\n",
    "encoded = Dense(64, activation=\"relu\")(encoded)\n",
    "encoded = LayerNormalization()(encoded)\n",
    "encoded = Dropout(0.2)(encoded)\n",
    "encoded = Dense(32, activation=\"relu\", name=\"embedding\")(encoded)\n",
    "\n",
    "# Decoder\n",
    "decoded = Dense(64, activation=\"relu\")(encoded)\n",
    "decoded = LayerNormalization()(decoded)\n",
    "decoded = Dropout(0.2)(decoded)\n",
    "decoded = Dense(128, activation=\"relu\")(decoded)\n",
    "decoded = LayerNormalization()(decoded)\n",
    "decoded = Dropout(0.2)(decoded)\n",
    "decoded = Dense(256, activation=\"relu\")(decoded)\n",
    "decoded = LayerNormalization()(decoded)\n",
    "decoded = Dropout(0.2)(decoded)\n",
    "\n",
    "# Final output with custom non-negative layer\n",
    "output_layer = Dense(input_dim)(decoded)\n",
    "output_layer = NonNegativeOutputLayer()(output_layer)  # Apply custom layer to enforce constraints\n",
    "\n",
    "# Model definition\n",
    "autoencoder = Model(input_layer, output_layer)\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=initial_lr), loss=\"mse\")\n",
    "\n",
    "# Callbacks\n",
    "history_file_path = cfg.HISTORY_FILE_PATH\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=1e-6)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_warmup = LearningRateScheduler(warmup, verbose=1)\n",
    "\n",
    "# Training with saving and loading history\n",
    "if cfg.OVERWRITE_AUTOENCODER or not os.path.exists(cfg.AUTOENCODER_MODEL_PATH):\n",
    "    print(\"Training autoencoder with enhanced configuration...\")\n",
    "    history = autoencoder.fit(\n",
    "        X_train, X_train,\n",
    "        epochs=cfg.AUTO_ENCODER_EPOCHS,\n",
    "        batch_size=cfg.AUTO_ENCODER_BATCH_SIZE,\n",
    "        validation_data=(X_test, X_test),\n",
    "        verbose=cfg.VERBOSE,\n",
    "        callbacks=[lr_warmup, lr_reduction, early_stopping]\n",
    "    )\n",
    "    autoencoder.save(cfg.AUTOENCODER_MODEL_PATH)\n",
    "\n",
    "    # Save training history to disk\n",
    "    with open(history_file_path, 'w') as f:\n",
    "        json.dump({k: [float(v) for v in values] for k, values in history.history.items()}, f)\n",
    "    history_data = history.history  # Save for plotting\n",
    "else:\n",
    "    print(\"Loading pre-trained autoencoder...\")\n",
    "    # autoencoder = load_model(cfg.AUTOENCODER_MODEL_PATH)\n",
    "    autoencoder = load_model(\n",
    "        cfg.AUTOENCODER_MODEL_PATH,\n",
    "        custom_objects={\"NonNegativeOutputLayer\": NonNegativeOutputLayer},\n",
    "        compile=False\n",
    "    )\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=initial_lr), loss=\"mse\")\n",
    "    with open(history_file_path, 'r') as f:\n",
    "        history_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ad265-291e-406e-9b83-14f68e3bfbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_long = pd.DataFrame({\n",
    "    'epoch': list(range(len(history_data['loss']))),\n",
    "    'training loss': history_data['loss'],\n",
    "    'validation loss': history_data['val_loss']\n",
    "}).melt(id_vars='epoch', var_name='type', value_name='loss_value')\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.set_context(\"talk\", font_scale=1.1)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.lineplot(\n",
    "    data=history_long,\n",
    "    x='epoch',\n",
    "    y='loss_value',\n",
    "    hue='type',\n",
    "    palette={'training loss': 'cornflowerblue', 'validation loss': 'red'}\n",
    ")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss (MSE)')\n",
    "# plt.title('Autoencoder Training and Validation Loss Over Epochs', loc='left')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "legend = plt.legend(title='', loc='upper right', frameon=False)\n",
    "\n",
    "# Save the plot to a JPEG file\n",
    "plt.savefig(\"autoencoder_loss_curves.jpg\", format=\"jpg\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd664ff-e7df-4a58-8f16-229ab225a8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values using the autoencoder\n",
    "print(\"Imputing missing values using the autoencoder...\")\n",
    "X_test_pred = autoencoder.predict(X_test)\n",
    "pred_df = pd.DataFrame(scaler.inverse_transform(X_test_pred), columns=df.columns, index=test_df.index)\n",
    "display(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a83b465",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.autograph.set_verbosity(0)  # Suppress retracing warnings\n",
    "\n",
    "\n",
    "def find_embedding_layer_by_name(model, layer_name=\"embedding\"):\n",
    "    for layer in model.layers:\n",
    "        if layer.name == layer_name:\n",
    "            return layer\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_embeddings(autoencoder, input_df, scaler):\n",
    "    \"\"\"\n",
    "    Scales the input dataframe using 'scaler', finds the embedding layer in the\n",
    "    'autoencoder', then creates a sub-model that outputs the embedding vectors.\n",
    "    Returns a DataFrame containing the embedding for each row of 'input_df'.\n",
    "    \"\"\"\n",
    "    # Normalize the entire input DataFrame\n",
    "    normalized_input = scaler.transform(input_df)\n",
    "\n",
    "    # Find the embedding layer\n",
    "    embedding_layer = find_embedding_layer_by_name(autoencoder, layer_name=\"embedding\")\n",
    "    if embedding_layer is None:\n",
    "        raise ValueError(\"No embedding layer found. Check model architecture or layer definitions.\")\n",
    "\n",
    "    # Create a sub-model from the autoencoder's input to the embedding layer's output\n",
    "    encoder_model = Model(inputs=autoencoder.input, outputs=embedding_layer.output)\n",
    "\n",
    "    # Predict the embeddings (suppressing verbose output)\n",
    "    embeddings = encoder_model.predict(normalized_input, verbose=0)\n",
    "    return pd.DataFrame(embeddings, index=input_df.index)\n",
    "\n",
    "\n",
    "# 1. Sample 3 rows of your data\n",
    "data = original_df.sample(3, random_state=1000)\n",
    "\n",
    "# 2. Extract embeddings\n",
    "embedding_vectors_df = get_embeddings(autoencoder, data, scaler)\n",
    "\n",
    "# 3. Display original data\n",
    "header('original data sample')\n",
    "display(HTML(data.to_html(index=False)))\n",
    "\n",
    "# 4. Split the embedding columns for display\n",
    "midpoint = len(embedding_vectors_df.columns) // 2\n",
    "first_half = embedding_vectors_df.iloc[:, :midpoint]\n",
    "second_half = embedding_vectors_df.iloc[:, midpoint:]\n",
    "\n",
    "# 5. Display the first half of the embedding columns\n",
    "header('autoencoder-generated embeddings (first half)')\n",
    "display(HTML(first_half.to_html(index=False)))\n",
    "\n",
    "# 6. Display the second half of the embedding columns\n",
    "header('autoencoder-generated embeddings (second half)')\n",
    "display(HTML(second_half.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1340ad14-0549-4ac7-9a37-8707cef86116",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.OVERWRITE_RF or not os.path.exists(cfg.RF_MODEL_PATH):\n",
    "\n",
    "    print(\"Imputing missing values using IterativeImputer with Random Forest...\")\n",
    "    \n",
    "    # Fit the imputer on X_train (complete cases only)\n",
    "    rf_imputer = IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=cfg.RANDOM_STATE)\n",
    "    rf_imputer.fit(X_train)  # Train only on non-missing data\n",
    "\n",
    "    # Save the fitted IterativeImputer with RandomForest\n",
    "    joblib.dump(rf_imputer, cfg.RF_MODEL_PATH)\n",
    "    print(\"Imputer model saved to disk.\")\n",
    "\n",
    "else:\n",
    "    print('Loading RF imputation model from disk')\n",
    "    rf_imputer = joblib.load(cfg.RF_MODEL_PATH)\n",
    "\n",
    "# Use the trained imputer to transform X_test, applying the learned imputation patterns\n",
    "rf_imputed_scaled = rf_imputer.transform(X_test)\n",
    "rf_imputed = pd.DataFrame(scaler.inverse_transform(rf_imputed_scaled), columns=df.columns, index=test_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df40c1d-fe0c-477c-8bdd-3ac29db9ff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute differences between imputed values and original values for the test set\n",
    "abs_diff_records = []\n",
    "\n",
    "for col in test_df.columns:\n",
    "    # Get indices in the test set where original data had NaNs\n",
    "    missing_indices = test_df.loc[test_df[col].isna(), col].index\n",
    "\n",
    "    # Extract the relevant values from original, autoencoder, and random forest imputed DataFrames\n",
    "    original_values = original_df.loc[missing_indices, col]\n",
    "    ae_imputed_values = pred_df.loc[missing_indices, col]\n",
    "    rf_imputed_values = rf_imputed.loc[missing_indices, col]\n",
    "    \n",
    "    # Calculate absolute differences for each model, avoiding potential NaN comparisons\n",
    "    ae_abs_diff = (ae_imputed_values - original_values).abs()\n",
    "    rf_abs_diff = (rf_imputed_values - original_values).abs()\n",
    "    \n",
    "    # Append results for plotting\n",
    "    abs_diff_records.extend([\n",
    "        {'Feature': col, 'Model': 'Autoencoder', 'Abs Difference': diff}\n",
    "        for diff in ae_abs_diff.dropna()\n",
    "    ])\n",
    "    abs_diff_records.extend([\n",
    "        {'Feature': col, 'Model': 'Random Forest', 'Abs Difference': diff}\n",
    "        for diff in rf_abs_diff.dropna()\n",
    "    ])\n",
    "\n",
    "# Convert to DataFrame for analysis and plotting\n",
    "abs_diff_df = pd.DataFrame(abs_diff_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a01d0e-6849-410f-93c1-8340b3d16c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a dark theme and define a color palette\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.set_context(\"talk\", font_scale=1.1)\n",
    "plt.rcParams['font.family'] = 'Avenir'\n",
    "custom_palette = {\"Autoencoder\": \"cornflowerblue\", \"Random Forest\": \"#ff7f0e\"}  # Vibrant colors for models\n",
    "\n",
    "# Create the box plot with enhanced styling\n",
    "golden_ratio = 1.6180339887\n",
    "plt.figure(figsize=(14, 14/golden_ratio))\n",
    "sns.boxplot(\n",
    "    data=abs_diff_df, \n",
    "    x='Feature', \n",
    "    y='Abs Difference', \n",
    "    hue='Model', \n",
    "    palette=custom_palette,\n",
    "    width=0.5,  # Narrower boxes\n",
    "    linewidth=1.5,  # Thicker lines for contrast\n",
    "    fliersize=3  # Smaller outliers\n",
    ")\n",
    "\n",
    "# Log scale for the y-axis and adjusted tick parameters\n",
    "fontsize=18\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(rotation=45, fontsize=fontsize)\n",
    "plt.yticks(color=\"black\")\n",
    "\n",
    "# Titles and labels with color adjustments\n",
    "# plt.title(\"Absolute Differences Between Imputed and Original Values by Feature and Model\", fontsize=fontsize*1.2, fontweight='bold', \n",
    "#           loc='left')\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"absolute difference (log scale)\", color=\"black\", fontsize=fontsize)\n",
    "plt.legend(title=\"\", fontsize=14, title_fontsize=12, facecolor='white', framealpha=0.8)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"ae_vs_rf_imputation_accuracy.jpg\", format=\"jpg\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64250b00-6355-4a07-8e39-9fc890aa3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_abs_diff_df = abs_diff_df.groupby(['Feature', 'Model']).median().reset_index()\n",
    "\n",
    "# Pivot the DataFrame for easier comparison\n",
    "pivot_df = mean_abs_diff_df.pivot(index='Feature', columns='Model', values='Abs Difference')\n",
    "\n",
    "# Calculate the ratio of the Abs Difference for Random Forest to Autoencoder\n",
    "pivot_df['RF/AE'] = pivot_df['Random Forest'] / pivot_df['Autoencoder']\n",
    "\n",
    "# Display the result with only the 'Ratio' column\n",
    "ratio_df = pivot_df[['RF/AE']].reset_index()\n",
    "ratio_df.columns.name = None\n",
    "\n",
    "ratio_df.sort_values('RF/AE', ascending=False, inplace=True)\n",
    "\n",
    "full_display(ratio_df.round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c51946f-9fd3-4d91-8f0d-5b4b6b0bd869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xyonix_autoencoder",
   "language": "python",
   "name": "xyonix_autoencoder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
