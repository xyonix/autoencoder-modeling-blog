{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fad5c5-e2cd-4f69-bbfd-fd0856299b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, Concatenate, ReLU\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers.legacy import Adam  # Use legacy optimizer for M1/M2 Macs\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, LearningRateScheduler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tensorflow.keras.layers import Masking\n",
    "import joblib\n",
    "from types import SimpleNamespace\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.saving import register_keras_serializable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5df8b4-5160-4f4a-a853-dc1bf9ce8d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR=os.path.abspath(os.path.expanduser(\"models\"))\n",
    "\n",
    "cfg = SimpleNamespace(\n",
    "    NA_FILL=-1,\n",
    "    AUTO_ENCODER_EPOCHS=100,\n",
    "    AUTO_ENCODER_BATCH_SIZE=32,\n",
    "    AUTO_ENCODER_LEARNING_RATE=0.001,\n",
    "    MODEL_DIR=MODEL_DIR,\n",
    "    AUTOENCODER_MODEL_PATH=os.path.join(MODEL_DIR, \"autoencoder_model.keras\"),\n",
    "    RF_MODEL_PATH=os.path.join(MODEL_DIR, \"rf_imputer.joblib\"),\n",
    "    OVERWRITE_AUTOENCODER=False,\n",
    "    OVERWRITE_RF=False,\n",
    "    VERBOSE=1,\n",
    "    RANDOM_STATE=100,\n",
    "    RANDOM_SAMPLE_FRACTION=0.2,\n",
    "    HISTORY_FILE_PATH=os.path.join(MODEL_DIR, \"autoencoder_training_history.json\")\n",
    ")\n",
    "\n",
    "os.makedirs(cfg.MODEL_DIR, exist_ok=True)\n",
    "\n",
    "cfg.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427e0a14-0d3e-4ab8-8b9a-b9f0df131603",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable()  # Register the custom layer\n",
    "class NonNegativeOutputLayer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        fixed_outputs = tf.nn.relu(inputs[:, :num_fixed_features])  # Enforce non-negative values for fixed features\n",
    "        unrestricted_outputs = inputs[:, num_fixed_features:]  # Allow unrestricted values for lat/lon\n",
    "        return tf.concat([fixed_outputs, unrestricted_outputs], axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()  # Get base config if any\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5dbf5c-9944-46ce-af3e-09c28756997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and simulate missing values\n",
    "data = fetch_california_housing()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# Save original values for comparison\n",
    "original_df = df.copy()\n",
    "\n",
    "# Introduce NaNs randomly, excluding Longitude and Latitude\n",
    "np.random.seed(cfg.RANDOM_STATE)\n",
    "nan_mask = np.random.rand(*df.shape) < cfg.RANDOM_SAMPLE_FRACTION\n",
    "nan_mask[:, df.columns.get_loc('Longitude')] = False  # Exclude Longitude\n",
    "nan_mask[:, df.columns.get_loc('Latitude')] = False   # Exclude Latitude\n",
    "df[nan_mask] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "# Separate rows with and without missing values\n",
    "train_df = df.dropna()\n",
    "test_df = df[df.isna().any(axis=1)]\n",
    "\n",
    "# Normalize and handle missing values\n",
    "df_filled = df.fillna(cfg.NA_FILL)\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_filled), columns=df.columns)\n",
    "\n",
    "# Scale train and test sets individually for model input\n",
    "X_train = scaler.transform(train_df.fillna(cfg.NA_FILL))\n",
    "X_test = scaler.transform(test_df.fillna(cfg.NA_FILL))\n",
    "\n",
    "# Define fixed and unrestricted features\n",
    "fixed_features = [col for col in df.columns if col not in [\"Longitude\", \"Latitude\"]]\n",
    "num_fixed_features = len(fixed_features)\n",
    "\n",
    "# Learning rate warm-up function\n",
    "initial_lr = 1e-5\n",
    "def warmup(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr + (initial_lr * 0.2)\n",
    "    return lr\n",
    "\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train.shape[1]\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "# Encoder\n",
    "encoded = Dense(256, activation=\"relu\")(input_layer)\n",
    "encoded = LayerNormalization()(encoded)\n",
    "encoded = Dropout(0.2)(encoded)\n",
    "encoded = Dense(128, activation=\"relu\")(encoded)\n",
    "encoded = LayerNormalization()(encoded)\n",
    "encoded = Dropout(0.2)(encoded)\n",
    "encoded = Dense(64, activation=\"relu\")(encoded)\n",
    "encoded = LayerNormalization()(encoded)\n",
    "encoded = Dropout(0.2)(encoded)\n",
    "encoded = Dense(32, activation=\"relu\")(encoded)\n",
    "\n",
    "# Decoder\n",
    "decoded = Dense(64, activation=\"relu\")(encoded)\n",
    "decoded = LayerNormalization()(decoded)\n",
    "decoded = Dropout(0.2)(decoded)\n",
    "decoded = Dense(128, activation=\"relu\")(decoded)\n",
    "decoded = LayerNormalization()(decoded)\n",
    "decoded = Dropout(0.2)(decoded)\n",
    "decoded = Dense(256, activation=\"relu\")(decoded)\n",
    "decoded = LayerNormalization()(decoded)\n",
    "decoded = Dropout(0.2)(decoded)\n",
    "\n",
    "# Final output with custom non-negative layer\n",
    "output_layer = Dense(input_dim)(decoded)\n",
    "output_layer = NonNegativeOutputLayer()(output_layer)  # Apply custom layer to enforce constraints\n",
    "\n",
    "# Model definition\n",
    "autoencoder = Model(input_layer, output_layer)\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=initial_lr), loss=\"mse\")\n",
    "\n",
    "# Callbacks\n",
    "history_file_path = cfg.HISTORY_FILE_PATH\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=1e-6)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_warmup = LearningRateScheduler(warmup, verbose=1)\n",
    "\n",
    "# Training with saving and loading history\n",
    "if cfg.OVERWRITE_AUTOENCODER or not os.path.exists(cfg.AUTOENCODER_MODEL_PATH):\n",
    "    print(\"Training autoencoder with enhanced configuration...\")\n",
    "    history = autoencoder.fit(\n",
    "        X_train, X_train,\n",
    "        epochs=cfg.AUTO_ENCODER_EPOCHS,\n",
    "        batch_size=cfg.AUTO_ENCODER_BATCH_SIZE,\n",
    "        validation_data=(X_test, X_test),\n",
    "        verbose=cfg.VERBOSE,\n",
    "        callbacks=[lr_warmup, lr_reduction, early_stopping]\n",
    "    )\n",
    "    autoencoder.save(cfg.AUTOENCODER_MODEL_PATH)\n",
    "\n",
    "    # Save training history to disk\n",
    "    with open(history_file_path, 'w') as f:\n",
    "        json.dump({k: [float(v) for v in values] for k, values in history.history.items()}, f)\n",
    "    history_data = history.history  # Save for plotting\n",
    "else:\n",
    "    print(\"Loading pre-trained autoencoder...\")\n",
    "    # autoencoder = load_model(cfg.AUTOENCODER_MODEL_PATH)\n",
    "    autoencoder = load_model(\n",
    "        cfg.AUTOENCODER_MODEL_PATH,\n",
    "        custom_objects={\"NonNegativeOutputLayer\": NonNegativeOutputLayer},\n",
    "        compile=False\n",
    "    )\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=initial_lr), loss=\"mse\")\n",
    "    with open(history_file_path, 'r') as f:\n",
    "        history_data = json.load(f)\n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history_data['loss'], label='Training Loss')\n",
    "plt.plot(history_data['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Autoencoder Training and Validation Loss Over Epochs')\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Impute missing values using the autoencoder\n",
    "print(\"Imputing missing values using the autoencoder...\")\n",
    "X_test_pred = autoencoder.predict(X_test)\n",
    "pred_df = pd.DataFrame(scaler.inverse_transform(X_test_pred), columns=df.columns, index=test_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f76f270-1c56-449d-9a5a-25cfba41427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1340ad14-0549-4ac7-9a37-8707cef86116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "print(\"Imputing missing values using IterativeImputer with Random Forest...\")\n",
    "\n",
    "# Fit the imputer on X_train (complete cases only)\n",
    "rf_imputer = IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=cfg.RANDOM_STATE)\n",
    "rf_imputer.fit(X_train)  # Train only on non-missing data\n",
    "\n",
    "# Use the trained imputer to transform X_test, applying the learned imputation patterns\n",
    "rf_imputed_scaled = rf_imputer.transform(X_test)\n",
    "rf_imputed = pd.DataFrame(scaler.inverse_transform(rf_imputed_scaled), columns=df.columns, index=test_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1898d55-a230-470b-8d2f-0dd6b53ce1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the fitted IterativeImputer with RandomForest\n",
    "joblib.dump(rf_imputer, 'rf_imputer_model.joblib')\n",
    "print(\"Imputer model saved to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2595ed-6818-4cc9-86c6-ebf1a9e5b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IterativeImputer from disk\n",
    "rf_imputer_loaded = joblib.load('rf_imputer_model.joblib')\n",
    "print(\"Imputer model loaded from disk.\")\n",
    "\n",
    "# Now you can use rf_imputer_loaded to transform new data\n",
    "new_imputed_data = rf_imputer_loaded.transform(X_test)\n",
    "new_imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f022315-d6fa-41ff-b0ad-e9ef508d7205",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test_df:')\n",
    "print(test_df)\n",
    "\n",
    "print('original_df:')\n",
    "print(original_df)\n",
    "\n",
    "print('pred_df:')\n",
    "print(pred_df)\n",
    "\n",
    "print('rf_imputed:')\n",
    "print(rf_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ead3a3-c9a5-4093-8d57-eb379de8af4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_indices = test_df[col].isna().index\n",
    "\n",
    "missing_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1255be52-7b30-4335-a7cd-a98cdebfab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'test_df.shape = {test_df.shape}')\n",
    "print(f'original_df.shape = {original_df.shape}')\n",
    "print(f'pred_df.shape = {pred_df.shape}')\n",
    "print(f'rf_imputed.shape = {rf_imputed.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a24ff8-c25b-482a-9f1c-6487c309c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.loc[test_df[col].isna(), col].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16aaf1a-b4b8-4c6d-9468-7430052bf461",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'MedInc'\n",
    "\n",
    "missing_indices = test_df.loc[test_df[col].isna(), col].index\n",
    "\n",
    "print(len(missing_indices))\n",
    "\n",
    "original_values = original_df.loc[missing_indices, col]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecd4585-a19d-425d-a997-8323ed8537bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.loc["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df40c1d-fe0c-477c-8bdd-3ac29db9ff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 7: Calculate absolute differences between imputed values and original values for the test set\n",
    "abs_diff_records = []\n",
    "\n",
    "for col in test_df.columns:\n",
    "    # Get indices in the test set where original data had NaNs\n",
    "    missing_indices = test_df.loc[test_df[col].isna(), col].index\n",
    "\n",
    "    # Extract the relevant values from original, autoencoder, and random forest imputed DataFrames\n",
    "    original_values = original_df.loc[missing_indices, col]\n",
    "    ae_imputed_values = pred_df.loc[missing_indices, col]\n",
    "    rf_imputed_values = rf_imputed.loc[missing_indices, col]\n",
    "    \n",
    "    # Calculate absolute differences for each model, avoiding potential NaN comparisons\n",
    "    ae_abs_diff = (ae_imputed_values - original_values).abs()\n",
    "    rf_abs_diff = (rf_imputed_values - original_values).abs()\n",
    "    \n",
    "    # Append results for plotting\n",
    "    abs_diff_records.extend([\n",
    "        {'Feature': col, 'Model': 'Autoencoder', 'Abs Difference': diff}\n",
    "        for diff in ae_abs_diff.dropna()\n",
    "    ])\n",
    "    abs_diff_records.extend([\n",
    "        {'Feature': col, 'Model': 'Random Forest', 'Abs Difference': diff}\n",
    "        for diff in rf_abs_diff.dropna()\n",
    "    ])\n",
    "\n",
    "# Convert to DataFrame for analysis and plotting\n",
    "abs_diff_df = pd.DataFrame(abs_diff_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a01d0e-6849-410f-93c1-8340b3d16c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a dark theme and define a color palette\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.set_context(\"talk\", font_scale=1.1)\n",
    "plt.rcParams['font.family'] = 'Avenir'\n",
    "custom_palette = {\"Autoencoder\": \"cornflowerblue\", \"Random Forest\": \"#ff7f0e\"}  # Vibrant colors for models\n",
    "\n",
    "# Create the box plot with enhanced styling\n",
    "golden_ratio = 1.6180339887\n",
    "plt.figure(figsize=(14, 14/golden_ratio))\n",
    "sns.boxplot(\n",
    "    data=abs_diff_df, \n",
    "    x='Feature', \n",
    "    y='Abs Difference', \n",
    "    hue='Model', \n",
    "    palette=custom_palette,\n",
    "    width=0.5,  # Narrower boxes\n",
    "    linewidth=1.5,  # Thicker lines for contrast\n",
    "    fliersize=3  # Smaller outliers\n",
    ")\n",
    "\n",
    "# Log scale for the y-axis and adjusted tick parameters\n",
    "fontsize=18\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(rotation=45, fontsize=fontsize)\n",
    "plt.yticks(color=\"black\")\n",
    "\n",
    "# Titles and labels with color adjustments\n",
    "plt.title(\"Absolute Differences Between Imputed and Original Values by Feature and Model\", fontsize=fontsize*1.2, fontweight='bold', \n",
    "          loc='left')\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"absolute difference (log scale)\", color=\"black\", fontsize=fontsize)\n",
    "plt.legend(title=\"\", fontsize=14, title_fontsize=12, facecolor='white', framealpha=0.8)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515610a8-9ac6-4780-9b3e-95bbaba35e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install graphviz pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca38fd5-e660-40ee-b5a5-06fdff7be8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(autoencoder, to_file='model_plot.png', show_shapes=True, show_layer_names=True, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e9d9ea-07d5-47d7-b934-24aba62c6eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes and edges\n",
    "nodes = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "edges = [(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"C\"), (\"C\", \"D\"), (\"D\", \"E\")]\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Define position layout (e.g., spring layout)\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "# Styling options for nodes and edges\n",
    "node_color = 'cornflowerblue'\n",
    "node_size = 1000\n",
    "edge_color = 'gray'\n",
    "edge_width = 1.5\n",
    "font_size = 12\n",
    "font_color = 'black'\n",
    "font_family = 'Avenir'\n",
    "\n",
    "# Draw the network graph with custom styling\n",
    "plt.figure(figsize=(10, 7))\n",
    "nx.draw_networkx(\n",
    "    G, pos,\n",
    "    node_color=node_color,\n",
    "    node_size=node_size,\n",
    "    edge_color=edge_color,\n",
    "    width=edge_width,\n",
    "    font_size=font_size,\n",
    "    font_color=font_color,\n",
    "    font_family=font_family,\n",
    "    with_labels=True,\n",
    "    connectionstyle='arc3,rad=0.1'  # Adds a slight curve to edges for visual appeal\n",
    ")\n",
    "\n",
    "# Additional styling options\n",
    "plt.title(\"Professional Network Graph\", loc='left', fontsize=16, fontweight='bold')\n",
    "plt.axis('off')  # Turn off the axis for a cleaner look\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3362da-c842-4b50-8e39-f1cc9e71c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "def plot_autoencoder_structure(model):\n",
    "    # Create a directed graph\n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    # Iterate over each layer in the model\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        layer_name = f\"{layer.name}\\n{layer.output_shape}\"\n",
    "        graph.add_node(layer_name)\n",
    "\n",
    "        if i > 0:\n",
    "            # Connect each layer to the previous one\n",
    "            prev_layer_name = f\"{model.layers[i - 1].name}\\n{model.layers[i - 1].output_shape}\"\n",
    "            graph.add_edge(prev_layer_name, layer_name)\n",
    "\n",
    "    # Draw the graph\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    pos = nx.spring_layout(graph)\n",
    "    nx.draw(graph, pos, with_labels=True, node_size=3000, node_color=\"skyblue\", font_size=10, font_weight=\"bold\")\n",
    "    plt.title(\"Autoencoder Architecture\")\n",
    "    plt.show()\n",
    "\n",
    "# Use this function to visualize your autoencoder\n",
    "plot_autoencoder_structure(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b93552-62a0-4658-aa16-7f40dde7878c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
